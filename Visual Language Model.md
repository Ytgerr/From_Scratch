LLM работает только с текстом -> нужна мультимодальность -> VLM

### мультимодальный pipeline
![alt text](image.png)
### Примеры использования
![alt text](image-1.png)
 VLM — это новая стадия развития области компьютерного зрения. Такие модели могут решать множество стандартных задач компьютерного зрения (классификация, детекция, описание и т. д.) в режимах zero‑shot и one‑shot. И пусть сейчас VLM показывает высокое качество не в каждой стандартной задаче, скорость развития таких моделей достаточно высокая.

### Архитектура 

В основном VLM состоят из трех частей:

1) LLM - текстовая модель, ничего не знающая про картинки
2) Картиночный энкодер — картиночная модель (CNN или ViT), ничего не знающая про тексты.
3) Adapter — модель, которая пытается сделать так, чтобы LLM и картиночный энкодер работали сообща

Пример инференса:
- подаём картинку на вход картиночного энкодера;
- преобразуем выход картиночного энкодера с помощью адаптера в некоторое представление;
- интегрируем выход адаптера в LLM (про то, как именно мы это делаем, описано ниже под катом);
- помимо обработки картинки, мы одновременно с этим преобразуем текстовый промпт в последовательность токенов и подаём их на вход LLM.

Модель адаптера:

- prompt‑based адаптеры;
	- Появились в статьях про модели [BLIP-2](https://arxiv.org/abs/2301.12597) и [LLaVa](https://arxiv.org/abs/2304.08485)
	- Берет выход картиночного энкодера (вектор, последовательность векторов, тензор — всё зависит от типа архитектуры) 
	- Преобразует в последовательность векторов (токенов), которые подает на вход LLM. 
	- В качестве такого адаптера может выступать просто MLP‑моделька с парой слоёв — даже такой простой адаптер хорошо показывает себя на практике.
	
- cross‑attention‑based адаптеры.
	- Появились в статьях про модели [Llama 3.2](https://arxiv.org/abs/2407.21783) и [NVLM](https://arxiv.org/abs/2409.11402)
	- Пытаются преобразовать выход картиночного энкодера таким образом, чтобы можно было его подать в cross‑attention‑блок LLM в качестве матриц key / value.
	- В качестве таких адаптеров могут выступать, например, трансформерные архитектуры типа [perceiver resampler](https://arxiv.org/abs/2204.14198) или [Q‑former](https://arxiv.org/abs/2301.12597).
![alt text](image-2.png)
	
Futher reading: [Multi-Modal Adapter](https://arxiv.org/html/2409.02958v1?), [good blog about VLM](https://rohitbandaru.github.io./blog/Vision-Language-Models/)

## Training 

VLM не учатся с нуля(пока что). За их основу берутся предобученные LLM и картиночный энкодер. Используя эти предобученные модели, дообучают VLM на мультимодальном картиночно‑текстовом домене.

![alt text](image-4.png)

Это дообучение состоит из нескольких этапов:
- pretraining;
- alignment: SFT + RL (опционально).

#### VLM pretraining
Цель:
- связать текстовую и картиночную модальности между собой (напомню, что у нас в модели есть адаптер, который мы не обучали раньше);
- загрузить в нашу модель знания о мире (при этом в картинках довольно много своей специфики — как минимум OCR‑навыки).

Eсть три вида претрейна, которые чаще всего используются при обучении VLM:

- **interleaved pretrain**. Это аналог LLM pretrain стадии, где мы учим модель на next‑token prediction задачу, подавая на вход веб‑документы. Здесь же в VLM pretrain стадии мы выбираем веб‑документы с картинками и учим модель предсказывать текст. При этом опирается она не только на текст, но и на те изображения, которые расположены на странице. Таких данных достаточно много, поэтому этот вид предобучения легко масштабировать. Однако качество данных тут довольно низкое: как именно его повышать — довольно большая и сложная задача.

![alt text](image-5.png)
- **Image‑text pairs pretrain**. Здесь мы учим модель на одну конкретную задачу — описывать изображения (image captioning). То есть нам нужен большой корпус картинок с релевантным к ним описанием. Такая задача более популярна, так как таких корпусов достаточно много — они используются при обучении других моделей (text‑to‑image генерация, image‑to‑text retrieval).

![alt text](image-6.png)
- instruct‑based pretrain. На этапе инференса в большинстве случаев мы будем подавать на вход модели не только картинку, но и текст.  В этом типе претрейна обучаются на большом количестве (иногда не самого лучшего качества) данных в виде троек «картинка‑инстракт‑ответ».

![alt text](image-7.png)

#### VLM alignment
Эта стадия обучение состоит из SFT‑обучения(Supervised Fine-Tuning) и дополнительной стадии RL.
Очень похожа на instruct‑based pretrain. Но здесь мы хотим сфокусироваться на данных очень хорошего качества, с хорошим форматированием и структурой ответа, а также с хорошими reasoning‑свойствами (то есть с возможностью не только понимать, что на картинке, но и с возможностью делать какие‑то выводы про изображение). А ещё в идеале не просесть в сценариях text‑only, где картинки не будет. Поэтому сюда же мы замешиваем и text‑only данные высокого качества.

Futher reading: [More about training](https://arxiv.org/html/2304.00685v2), [contrastive learning](https://arxiv.org/abs/2502.13928), [RLHF](https://aclanthology.org/2024.findings-acl.775), [Aligment](https://www.emergentmind.com/topics/vision-language-alignment)

Also need to know: метрики, ограничения, примеры моделей и примеры использования 
